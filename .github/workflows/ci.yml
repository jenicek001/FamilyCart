# CI/CD Pipeline for FamilyCart
# 
# This workflow requires the following GitHub Environment configurations:
#
# UAT Environment (repository Settings ‚Üí Environments ‚Üí uat):
#   - UAT_HOST (optional): Remote UAT server hostname/IP
#   - UAT_USER (optional): Username for remote UAT server  
#   - UAT_SSH_KEY (optional): SSH private key for remote UAT server
#   - UAT_BASE_URL (optional): UAT frontend URL (default: http://localhost:3001)
#   - UAT_API_URL (optional): UAT backend URL (default: http://localhost:8001)
#
# Production Environment (repository Settings ‚Üí Environments ‚Üí production):
#   - PRODUCTION_SSH_KEY: SSH private key for production server
#   - PRODUCTION_HOST: Production server hostname/IP
#   - PRODUCTION_USER: Username for production server
#   - PRODUCTION_URL: Production application URL
#
# Note: If UAT_HOST/UAT_USER/UAT_SSH_KEY are not set, UAT deployment will run locally on the self-hosted runner

name: CI/CD

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: jenicek001/familycart

jobs:
  test:
    runs-on: self-hosted

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Start database services with Docker Compose
        run: |
          echo "üê≥ Starting PostgreSQL and Redis services for testing..."
          
          # Check if Docker is accessible - use sudo for containerized runners
          if sudo docker info &> /dev/null; then
            echo "‚úÖ Docker daemon is accessible via sudo"
            DOCKER_CMD="sudo docker"
            COMPOSE_CMD="sudo docker-compose"
          elif docker info &> /dev/null; then
            echo "‚úÖ Docker daemon is accessible directly"
            DOCKER_CMD="docker"
            COMPOSE_CMD="docker-compose"
          else
            echo "‚ùå Docker daemon not accessible"
            exit 1
          fi
          
          # Start database services
          echo "ÔøΩ Starting test database services with Docker Compose..."
          $COMPOSE_CMD -f docker-compose.ci.yml down --remove-orphans || true
          $COMPOSE_CMD -f docker-compose.ci.yml up -d
          
          echo "‚è≥ Waiting for services to be ready..."
          sleep 30
          
          # Check service status
          echo "üìä Database services status:"
          $COMPOSE_CMD -f docker-compose.ci.yml ps
          
          # Test connectivity
          echo "ÔøΩ Testing database connectivity..."
          for i in {1..10}; do
            if $DOCKER_CMD exec postgres-ci-familycart pg_isready -U test_user -d test_familycart; then
              echo "‚úÖ PostgreSQL is ready!"
              break
            fi
            echo "PostgreSQL not ready, waiting... ($i/10)"
            sleep 3
          done
          
          # Test Redis connectivity
          for i in {1..5}; do
            if $DOCKER_CMD exec redis-ci-familycart redis-cli ping | grep -q PONG; then
              echo "‚úÖ Redis is ready!"
              break
            fi
            echo "Redis not ready, waiting... ($i/5)"
            sleep 2
          done
          
          # Set environment variable for cleanup
          echo "DATABASE_SERVICES_STARTED=true" >> $GITHUB_ENV
          echo "DOCKER_CMD=$DOCKER_CMD" >> $GITHUB_ENV
          echo "COMPOSE_CMD=$COMPOSE_CMD" >> $GITHUB_ENV

      - name: Verify database services are running
        run: |
          echo "üîç Verifying database services are healthy..."
          
          # Use the Docker commands from previous step
          DOCKER_CMD="${DOCKER_CMD:-sudo docker}"
          COMPOSE_CMD="${COMPOSE_CMD:-sudo docker-compose}"
          
          # Check PostgreSQL health
          echo "üêò Checking PostgreSQL health..."
          $DOCKER_CMD exec postgres-ci-familycart pg_isready -U test_user -d test_familycart
          
          # Check Redis health  
          echo "ÔøΩ Checking Redis health..."
          $DOCKER_CMD exec redis-ci-familycart redis-cli ping
          
          # Verify network connectivity
          echo "üåê Testing network connectivity..."
          echo "PostgreSQL container IP:"
          $DOCKER_CMD inspect postgres-ci-familycart --format='{{range.NetworkSettings.Networks}}{{.IPAddress}}{{end}}'
          echo "Redis container IP:"
          $DOCKER_CMD inspect redis-ci-familycart --format='{{range.NetworkSettings.Networks}}{{.IPAddress}}{{end}}'
          
          # Show running containers
          echo "ÔøΩ Running database containers:"
          $COMPOSE_CMD -f docker-compose.ci.yml ps
          
          echo "‚úÖ Database services verification completed!"

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'
      
      - name: Install PostgreSQL client
        run: |
          echo "üì¶ Installing PostgreSQL and Redis client tools..."
          sudo apt-get update
          sudo apt-get install -y postgresql-client redis-tools
          echo "‚úÖ Database client tools installed!"

      - name: Install Poetry
        uses: snok/install-poetry@v1
        with:
          version: 2.0.0
          virtualenvs-create: true
          virtualenvs-in-project: true

      - name: Load cached venv
        id: cached-poetry-dependencies
        uses: actions/cache@v4
        with:
          path: backend/.venv
          key: venv-${{ runner.os }}-${{ hashFiles('**/poetry.lock') }}-with-dev

      - name: Install backend dependencies
        if: steps.cached-poetry-dependencies.outputs.cache-hit != 'true'
        working-directory: ./backend
        run: |
          echo "üì¶ Installing backend dependencies with code quality tools..."
          poetry install --no-interaction --no-root --with dev
          echo "‚úÖ Dependencies installed successfully!"

      - name: Run backend code quality checks
        working-directory: ./backend
        run: |
          echo "üîç Running code quality checks with balanced standards..."
          
          echo "=== Black Code Formatting ==="
          poetry run black --check .
          
          echo "=== Import Sorting ==="
          poetry run isort --check-only .
          
          echo "=== Pylint Code Quality ==="
          # Use balanced approach: check score but allow warnings
          PYLINT_OUTPUT=$(poetry run pylint app/ --rcfile=pyproject.toml --score=y 2>&1) || PYLINT_EXIT_CODE=$?
          echo "$PYLINT_OUTPUT"
          
          # Extract score from output
          PYLINT_SCORE=$(echo "$PYLINT_OUTPUT" | grep -oP "Your code has been rated at \K[\d\.]+" || echo "0")
          echo "üìä Pylint score: $PYLINT_SCORE/10"
          
          # Check if score meets our threshold (9.0/10) using Python
          SCORE_CHECK=$(python3 -c "print('pass' if float('$PYLINT_SCORE') >= 9.0 else 'fail')")
          if [ "$SCORE_CHECK" = "pass" ]; then
            echo "‚úÖ Pylint quality check passed: $PYLINT_SCORE/10 (threshold: 9.0/10)"
          else
            echo "‚ùå Pylint quality check failed: $PYLINT_SCORE/10 (threshold: 9.0/10)"
            exit 1
          fi
          
          echo "‚úÖ Code quality checks completed successfully!"
          
      - name: Run backend security scan
        working-directory: ./backend
        run: |
          echo "üîí Running security scan with balanced criteria..."
          
          # Run bandit with our configured standards
          poetry run bandit -c pyproject.toml -r app/
          
          # Optional: Generate report for analysis (don't fail on issues)
          poetry run bandit -c pyproject.toml -r app/ -f json -o bandit-report.json || true
          
          echo "‚úÖ Security scan completed successfully!"

      - name: Generate code quality report
        if: always()
        working-directory: ./backend
        run: |
          echo "üìä Generating comprehensive code quality report..."
          
          # Generate detailed pylint report
          poetry run pylint app/ --rcfile=pyproject.toml --output-format=json > pylint-report.json || true
          poetry run pylint app/ --rcfile=pyproject.toml --output-format=text > pylint-report.txt || true
          
          # Generate coverage report if not already done
          if [ ! -f coverage.xml ]; then
            poetry run pytest --cov=app --cov-report=xml --cov-report=html > /dev/null 2>&1 || true
          fi
          
          echo "‚úÖ Code quality report generated!"

      - name: Upload code quality artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: backend-code-quality-reports
          path: |
            backend/pylint-report.json
            backend/pylint-report.txt
            backend/bandit-report.json
            backend/coverage.xml
            backend/htmlcov/
          retention-days: 30

      - name: Code quality summary
        if: always()
        working-directory: ./backend
        run: |
          echo "üìã Code Quality Summary:"
          echo "======================"
          
          # Extract pylint score if available
          if [ -f pylint-report.txt ]; then
            PYLINT_SCORE=$(grep -oP "Your code has been rated at \K[\d\.]+" pylint-report.txt || echo "N/A")
            echo "üîç Pylint Score: $PYLINT_SCORE/10"
          fi
          
          # Check if bandit found issues
          if [ -f bandit-report.json ]; then
            BANDIT_ISSUES=$(python3 -c "import json; data=json.load(open('bandit-report.json')); print(len(data.get('results', [])))" 2>/dev/null || echo "0")
            echo "üîí Security Issues: $BANDIT_ISSUES"
          fi
          
          # Coverage info
          if [ -f coverage.xml ]; then
            COVERAGE=$(python3 -c "import xml.etree.ElementTree as ET; tree=ET.parse('coverage.xml'); print(f'{float(tree.getroot().attrib[\"line-rate\"])*100:.1f}%')" 2>/dev/null || echo "N/A")
            echo "üìä Test Coverage: $COVERAGE"
          fi
          
          echo "======================"
          echo "‚úÖ Quality checks completed with balanced standards!"
          echo "üìÑ Reports available in artifacts"

      - name: Run backend tests with coverage
        working-directory: ./backend
        env:
          # Test database configuration - PostgreSQL for production parity
          POSTGRES_SERVER: localhost
          POSTGRES_USER: test_user
          POSTGRES_PASSWORD: test_password
          POSTGRES_DB: test_familycart
          POSTGRES_PORT: 5432
          # App configuration for tests
          SECRET_KEY: test-secret-key-for-ci
          ALGORITHM: "HS256"
          ACCESS_TOKEN_EXPIRE_MINUTES: "30"
          REDIS_URI: "redis://localhost:6379"
          CORS_ORIGINS: '["http://localhost:3000"]'
        run: |
          echo "üß™ Running backend tests with PostgreSQL..."
          
          # Wait for PostgreSQL to be ready using pg_isready
          echo "‚è≥ Waiting for PostgreSQL to be ready..."
          for i in {1..30}; do
            if pg_isready -h localhost -p 5432 -U test_user; then
              echo "‚úÖ PostgreSQL is ready!"
              break
            fi
            echo "PostgreSQL not ready, waiting... ($i/30)"
            sleep 2
          done
          
          # Test database connection
          echo "üîå Testing database connection..."
          PGPASSWORD=test_password psql -h localhost -U test_user -d test_familycart -c "SELECT 1;" || {
            echo "‚ùå Failed to connect to PostgreSQL"
            exit 1
          }
          
          # Run database migrations if needed
          echo "üîÑ Running database migrations..."
          poetry run alembic upgrade head || echo "‚ö†Ô∏è No migrations to run"
          
          # Run tests with coverage
          echo "üß™ Running pytest with coverage..."
          poetry run pytest --cov=app --cov-report=xml --cov-report=html
          
      - name: Upload backend coverage to codecov
        uses: codecov/codecov-action@v4
        with:
          file: ./backend/coverage.xml
          flags: backend
          name: backend-coverage
          
      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
          cache-dependency-path: frontend/package-lock.json

      - name: Install frontend dependencies
        working-directory: ./frontend
        run: npm ci

      - name: Run frontend code quality checks
        working-directory: ./frontend
        run: |
          npm run lint
          npm run typecheck

      - name: Build frontend
        working-directory: ./frontend
        run: npm run build

      - name: Run frontend tests
        working-directory: ./frontend
        run: npm test -- --coverage --watchAll=false

  security-scan:
    runs-on: self-hosted
    needs: test
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Run Trivy vulnerability scanner
        uses: aquasecurity/trivy-action@master
        with:
          scan-type: 'fs'
          scan-ref: '.'
          format: 'sarif'
          output: 'trivy-results.sarif'

      - name: Upload Trivy scan results to GitHub Security
        uses: github/codeql-action/upload-sarif@v3
        with:
          sarif_file: 'trivy-results.sarif'

  build:
    runs-on: self-hosted
    needs: [test, security-scan]
    outputs:
      backend-image: ${{ steps.meta-backend.outputs.tags }}
      frontend-image: ${{ steps.meta-frontend.outputs.tags }}
      backend-digest: ${{ steps.build-backend.outputs.digest }}
      frontend-digest: ${{ steps.build-frontend.outputs.digest }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Log in to Container Registry
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Extract backend metadata
        id: meta-backend
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}-backend
          tags: |
            type=ref,event=branch
            type=ref,event=pr
            type=sha,prefix={{branch}}-
            type=raw,value=latest,enable={{is_default_branch}}

      - name: Build and push backend image
        id: build-backend
        uses: docker/build-push-action@v5
        with:
          context: ./backend
          push: true
          tags: ${{ steps.meta-backend.outputs.tags }}
          labels: ${{ steps.meta-backend.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max
          platforms: linux/amd64

      - name: Extract frontend metadata
        id: meta-frontend
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}-frontend
          tags: |
            type=ref,event=branch
            type=ref,event=pr
            type=sha,prefix={{branch}}-
            type=raw,value=latest,enable={{is_default_branch}}

      - name: Build and push frontend image
        id: build-frontend
        uses: docker/build-push-action@v5
        with:
          context: ./frontend
          push: true
          tags: ${{ steps.meta-frontend.outputs.tags }}
          labels: ${{ steps.meta-frontend.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max
          platforms: linux/amd64

  deploy-uat:
    runs-on: self-hosted
    needs: build
    if: github.ref == 'refs/heads/develop'
    environment: uat
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Deploy to UAT environment
        env:
          BACKEND_IMAGE: ${{ needs.build.outputs.backend-image }}
          FRONTEND_IMAGE: ${{ needs.build.outputs.frontend-image }}
          UAT_HOST: ${{ secrets.UAT_HOST }}
          UAT_USER: ${{ secrets.UAT_USER }}
          UAT_SSH_KEY: ${{ secrets.UAT_SSH_KEY }}
        run: |
          # Prepare SSH key if deploying to remote UAT
          if [ -n "$UAT_SSH_KEY" ] && [ -n "$UAT_HOST" ]; then
            echo "$UAT_SSH_KEY" > /tmp/uat_key
            chmod 600 /tmp/uat_key
            
            # Deploy to remote UAT server
            ssh -i /tmp/uat_key -o StrictHostKeyChecking=no $UAT_USER@$UAT_HOST << 'EOF'
              cd /opt/familycart-uat
              
              # Update docker-compose with new images
              sed -i "s|ghcr.io/jenicek001/familycart-backend:.*|$BACKEND_IMAGE|g" docker-compose.uat.yml
              sed -i "s|ghcr.io/jenicek001/familycart-frontend:.*|$FRONTEND_IMAGE|g" docker-compose.uat.yml
              
              # Pull new images and deploy
              docker-compose -f docker-compose.uat.yml pull
              docker-compose -f docker-compose.uat.yml up -d --remove-orphans
              
              # Wait for services to start
              sleep 60
          EOF
            
            # Cleanup SSH key
            rm /tmp/uat_key
          else
            # Local UAT deployment (self-hosted runner)
            cd /opt/familycart-uat
            
            # Update docker-compose with new images
            sed -i "s|ghcr.io/jenicek001/familycart-backend:.*|${BACKEND_IMAGE}|g" docker-compose.uat.yml
            sed -i "s|ghcr.io/jenicek001/familycart-frontend:.*|${FRONTEND_IMAGE}|g" docker-compose.uat.yml
            
            # Pull new images and deploy
            docker-compose -f docker-compose.uat.yml pull
            docker-compose -f docker-compose.uat.yml up -d --remove-orphans
            
            # Wait for services to start
            sleep 60
          fi
          
      - name: Health check UAT deployment
        env:
          UAT_BASE_URL: ${{ secrets.UAT_BASE_URL || 'http://localhost:3001' }}
          UAT_API_URL: ${{ secrets.UAT_API_URL || 'http://localhost:8001' }}
        run: |
          # Check backend health
          curl -f ${UAT_API_URL}/health || (echo "Backend health check failed" && exit 1)
          
          # Check frontend availability  
          curl -f ${UAT_BASE_URL}/ || (echo "Frontend health check failed" && exit 1)
          
          # Check database connectivity (if local deployment)
          if [ -z "$UAT_HOST" ]; then
            docker exec familycart-uat-db pg_isready -U familycart_uat -d familycart_uat || (echo "Database health check failed" && exit 1)
          fi

      - name: Run UAT integration tests
        working-directory: ./frontend
        env:
          UAT_BASE_URL: ${{ secrets.UAT_BASE_URL || 'http://localhost:3001' }}
          UAT_API_URL: ${{ secrets.UAT_API_URL || 'http://localhost:8001' }}
        run: |
          # Run basic integration tests against UAT environment
          npm run test:integration:uat || echo "Integration tests failed but deployment continues"

      - name: Notify deployment success
        if: success()
        run: |
          echo "‚úÖ UAT deployment successful!"
          echo "UAT environment is available at: http://uat.familycart.local"
          echo "Backend: ${{ needs.build.outputs.backend-image }}"
          echo "Frontend: ${{ needs.build.outputs.frontend-image }}"

  deploy-production:
    runs-on: self-hosted
    needs: [build, deploy-uat]
    if: github.ref == 'refs/heads/main'
    environment: production
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Deploy to Production
        env:
          PRODUCTION_SSH_KEY: ${{ secrets.PRODUCTION_SSH_KEY }}
          PRODUCTION_HOST: ${{ secrets.PRODUCTION_HOST }}
          PRODUCTION_USER: ${{ secrets.PRODUCTION_USER }}
          BACKEND_IMAGE: ${{ needs.build.outputs.backend-image }}
          FRONTEND_IMAGE: ${{ needs.build.outputs.frontend-image }}
        run: |
          # Prepare SSH key
          echo "$PRODUCTION_SSH_KEY" > /tmp/production_key
          chmod 600 /tmp/production_key
          
          # Deploy to production server
          ssh -i /tmp/production_key -o StrictHostKeyChecking=no $PRODUCTION_USER@$PRODUCTION_HOST << EOF
            cd /opt/familycart
            
            # Update image references
            sed -i "s|ghcr.io/jenicek001/familycart-backend:.*|$BACKEND_IMAGE|g" docker-compose.app.yml
            sed -i "s|ghcr.io/jenicek001/familycart-frontend:.*|$FRONTEND_IMAGE|g" docker-compose.app.yml
            
            # Deploy with zero downtime
            docker-compose -f docker-compose.app.yml pull
            docker-compose -f docker-compose.app.yml up -d --remove-orphans
            
            # Health check
            sleep 30
            curl -f http://localhost:8000/health || exit 1
          EOF
          
          # Cleanup SSH key
          rm /tmp/production_key

      - name: Production health check
        env:
          PRODUCTION_URL: ${{ secrets.PRODUCTION_URL }}
        run: |
          # Wait for deployment to stabilize
          sleep 60
          
          # Check production endpoint
          curl -f $PRODUCTION_URL/health || (echo "Production health check failed" && exit 1)
          
          # Optional: Run production smoke tests
          curl -f $PRODUCTION_URL/ || (echo "Production frontend check failed" && exit 1)

  performance-test:
    runs-on: self-hosted
    needs: deploy-uat
    if: github.ref == 'refs/heads/develop'
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Wait for UAT stabilization
        run: sleep 120
        
      - name: Run load tests against UAT
        run: |
          # Install k6 if not available
          if ! command -v k6 &> /dev/null; then
            sudo apt-get update
            sudo apt-get install -y gnupg software-properties-common
            sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys C5AD17C747E3415A3642D57D77C6C491D6AC1D69
            echo "deb https://dl.k6.io/deb stable main" | sudo tee /etc/apt/sources.list.d/k6.list
            sudo apt-get update
            sudo apt-get install k6
          fi
          
          # Run performance tests
          cd scripts
          k6 run --env UAT_BASE_URL=http://localhost:3001 load-test.js
          
      - name: Archive performance results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: performance-results
          path: scripts/results/
          retention-days: 30

  cleanup:
    runs-on: self-hosted
    needs: [deploy-uat, deploy-production, performance-test]
    if: always()
    steps:
      - name: Clean up test services
        if: always()
        run: |
          echo "üßπ Cleaning up test services..."
          
          # Clean up Docker Compose test services if they were started
          if [ "$DATABASE_SERVICES_STARTED" = "true" ]; then
            echo "üê≥ Stopping Docker Compose test services..."
            COMPOSE_CMD="${COMPOSE_CMD:-sudo docker-compose}"
            $COMPOSE_CMD -f docker-compose.ci.yml down --remove-orphans --volumes || true
            echo "‚úÖ Docker Compose test services cleaned up"
          fi
          
      - name: Clean up Docker resources
        run: |
          # Check if Docker is available before attempting cleanup
          if command -v docker &> /dev/null && docker info &> /dev/null; then
            echo "Docker is available, performing cleanup..."
            # Remove old images (keep last 5 versions)
            docker image prune -af --filter "until=168h" || echo "Docker image prune failed, continuing..."
            
            # Clean up build cache
            docker buildx prune -af --filter "until=168h" || echo "Docker buildx prune failed, continuing..."
            
            echo "‚úÖ Docker cleanup completed"
          else
            echo "‚ö†Ô∏è  Docker not available or daemon not running, skipping Docker cleanup"
          fi
          
      - name: System maintenance
        run: |
          # Clean up logs
          sudo journalctl --vacuum-time=7d
          
          # Update system packages
          sudo apt-get update && sudo apt-get upgrade -y